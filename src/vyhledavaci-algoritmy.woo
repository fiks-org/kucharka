.Chapter Řadící a vyhledávací algoritmy
  label: chap-radici-vyhledavaci-algo


Všichni máme zkušenosti ze života, kdy jsme něco hledali v nějaké tabulce, nebylo to seřazené
a my jsme si přáli, aby byla seřazená a nemuseli jsme procházet všechno. Pročítače to mají podobně
hodně algoritmů je mnohem efektivnějších, když mají vstup seřazený. Ale jak ho seřadíme?


V této kapitole se podíváme na několik řaďících algoritmů a poté i na nějaké vyhledávací, ať už v seřazeném
nebo neseřazeném vstupu. Také si znovu připomeneme počítání složitostí a ukážeme si jak se dokazují zbylé dvě
vlastnosti algoritmů, korektnost a konečnost.


.Section Řadící algoritmy
  label: sec-radici-algo

Pro řazení je důležité, aby pro prvky, které chceme seřadit existovalo nějaké "úplné uspořádání".reference.1,
to je zjednodušeně operátor~<, kterým můžeme libovolné dva prvky porovnat.
  1:
    link: https://cs.wikipedia.org/wiki/Line%C3%A1rn%C3%AD_uspo%C5%99%C3%A1d%C3%A1n%C3%AD


U řadících algoritmů nás navíc zajímá ještě jedna vlastnost a tou je "stabilita".notion.1. Mějme vstupní posloupnost
$p_1, p_2, \dots, p_n$. Řadící algoritmus je stabilní, pokud pro každé dva prvky $p_i$ a $p_j$, které si jsou rovny platí,
že jejich vzájemné pořadí na výstupu se je stejné jako jejich pořadí na vstupu.
Jinými slovy, pokud $p_i = p_j$ a $i < j$, pak se $p_i$ na výstupu objeví před $p_j$.


.Procedure:
  title: Select sort
  label: algo-select-sort
  index: sort!select sort

  Jeden z nejjednodušších algoritmů na seřazení je select sort. Princip spočívá v tom, že z posloupnosti
  vybírám ten nejmenší prvek a zařadím ho na konec seřazené části. Intuitivně to dává smysl, my si ukážeme,
  jak dokázat, že výsledek bude opravdu seřazený formálně.


  V každém kroku máme dvě, potenciálně prázdné, posloupnosti, vstupní $p$ a výstupní $s$. Jeden krok spočívá
  ve vybrání minimálního prvku, říkejme mu $p_m$ z $p$ a vložení na konec $s$. $p_m$ bude určitě větší nebo roven
  všem z $s$, protože všechny prvky $s$ byly někdy minimální v $p$, jinak se do $s$ dostat nemohli. Kdyby $p_m$ nebyl
  všetší nebo roven všem prvkům z $s$, tak to znamená, že existuje $s_x$ z $s$ takový, že $p_m$ < $s_x$. Ale to je spor s tím,
  že $s_x$ byl minimální v $p$. Takže $p_m$ je větší než všechny prvky v $s$ a patří na konec.


  Dobře je zde i vidět, že algoritmus skončí, každý krok zmenší posloupnost $p$ o jedna a protože je konečná,
  tak bude někdy prázdná a všechny její prvky budou v $s$, což bude seřazený výstup.


  Implementace by pak mohla vypadat v pseudokódu takhle, všimněte si, že se nepřesouvají prvky z jednoho pole
  do druhého, ale řadí se inplace. Je to určité šetření časem a místem. Jako cvičení si zkuste navrhnout implementaci,
  která by postupoval podle předchozího popisu a přesouvala prvky z jedné posloupnosti do druhé.

  !pseudocode:

    \PROCEDURE{SelectSort}{$p_1, p_2, \dots, p_n$}
      \FOR {$i = 1, \dots, n $ }
        \STATE {$m = i$}
        \FOR {$j = i+1, \dots, n $ }
          \IF {$p_j < p_m$}
            \STATE $m = j$
          \ENDIF
        \ENDFOR
        \STATE swap($p_i$, $p_m$)
      \ENDFOR
    \ENDPROCEDURE


  S touto implementací se můžeme podívat více na složitosti. Navíc oproti vstupu zde používáme jednu proměnou pro držení minima
  a další ve swapu, to znamená že poměťová složitost je $\lanO(1)$ navíc oproti vstupu. Časová složitost je trochu složitější výpočet,
  vnější forloop se provede $n$-krát, ale vnitřní závisí na hodnotě $i$, která se každou iteraci mění. A provede se tedy $(n-i)$-krát.
  Všechny operace (porovnání a swap) v loopu trvají konstatní dobu. Takže počet kroků můžeme napsat jako
  $\sum_{i=1}^{n} n-i$. To je ale jednoduchá aritmetická řada, jejíž součet můžeme napsat jako
  $\frac{n}{2}*(n-1) = \frac{n^2-n}{2}$, což je zhora omezeno $n^2$, takže časová složitost bude $\lanO(n^2)$.


  Zůstává nám zodpovědět, jestli je algoritmus stabilní nebo ne. Zkuste si to nejdřív sami.


.Solution:
  title: Stabilita select sortu
  lable: sol-select-sort-stability

  Na tuto otázku neexistuje jednoznačná odpověď, takže správná je "záleží".emphasize.
  Záleží na implementaci hledání minima, pseudokód v tomto textu je napsán tak, aby algoritmus stabilní byl,
  existuje však způsob nalezení minima, který stabilní nebude.


.Procedure:
  title: Bubble sort
  label: algo-bubble-sort
  index: sort!bubble sort

  Bublání je častá technika používaná v algoritmech, tento algoritmus funguje podobně, jako když máte mísu s různě velkými kamínky
  a zatřepete s ní a pomalu malé kamínky propadají do spod, zatímco ty větší zůstanou na vrchu.


  Algoritmus opakovaně prochází celým polem a porovnává sousední dvojice prvků. Pokud pro dvojici prvků $p_i$ a $p_{i+1}$ platí $p_{i} > p_{i+1}$, tak
  se prvky prohodí. Tím se postupně větší prvky přesouvají doprava a menší doleva. Po dostatečně mnoha průchodech pak bude pole seřazené.

  
  Bude tohle opravdu fungovat? Zkuste si sami dokázat korektnost bubble sortu - tedy, že nakonci bude pole skutečně seřazené,
  a zkuste vymyslet jakou bude mít náš algoritmus časovou a paměťovou složitost.


.Solution:
  title: Korektnost bubble sortu
  lable: sol-bubble-sort-correctness

  Všimněme si, že po prvním průchodu algoritmus posune největší prvek na konec pole, pokud na této pozici už nebyl. Po druhém průchodu posune druhý největší prvek
  na předposlední místo v poli, pokud tam už nebyl. Takže obecně po $k$-tém průchodu je na konci pole správně seřazených alespoň $k$ největších prvků. Pokud pole má
  $n$ prvků, pak po $n$ iteracích bude na konci pole správně seřazených alespoň $n$ největších prvků, jinak řečeno, pole bude seřazené.

  Tedy pokud chceme, aby bylo seřazeno $n$ prvků, tak to bude trvat nanejvíš $n$ průchodů polem - průchodů může být ale potřeba daleko méně, např. pokud začneme hned se správně
  seřazeným polem (dokážete najít nějaké pole, které bude potřebovat všech $n$ průchodů?). Takže algoritmus skončí a jeho výsledkem bude seřazené pole.


.Solution:
  title: Časová složitost bubble sortu
  lable: sol-bubble-sort-time-complexity

  Už víme, že průchodů bude v nejhorším případě $n$. Takže nás zajímá, jak dlouho trvá jeden průchod. Při jednom prchůchodu se každý prvek porovná se
  sousedem, takových porovnání bude $n-1$. Takže výsledná časová složitost je $\lanO(n^2)$. Protože víme, že $k$-tý průchod bude posledních $k$ prvků seřezených, tak můžeme
  algoritmus trochu optimalizovat a porovnávat jenom prvních $n-k$ prvků. Takže pak máme $\sum_{k=0}^{n-1} n-k = \frac{n(n+1)}{2} = \lanO(n^2)$.


.Solution:
  title: Prostorová složitost bubble sortu
  lable: sol-bubble-sort-space-complexity

  Až na pár pomocných proměných (iterační nebo při prohazování) nepotřebuje algortmus žádnou paměť navíc, takže paměťová složitost je $\lanO(1)$.


Hurá! Nyní už víme jak řadit a dokonce známe rovnou dva algoritmy řazení. Oba algoritmy běží v nejhorším případě v čase $\lanO(n^2)$. Je to dobré? Je to špatné?
Na tyto otázky obecně nejde moc dobře odpovědět, pokud neznáme vstupní data. Správně bychom si měli spíš položit otázku: Existují i reychlejší algoritmy?


Odpověď na tuto otázku je pozitivní a v následující sekci vám představíme jeden známý algoritmus řazení pracující v (asymptoticky) rychlejším čase.
Půjde o algoritmus pracující na bázi \textit{rozděl a panuj} (anglicky \textit{divide and conquer}), o které se obecně pobavíme později. Prozatím stačí vědět, že celý princip
spočívá v rozdělení původního problému na menší celky, které umíme řešit, a následně pak jednotlivá řešení spojit dohromady. Předtím než algoritmus uvedeme, ukážeme si princip na méně
známem řadícím algoritmu.


.Procedure:
  title: Marakuja sort
  index: sort!marakuja sort

  Princip Marakuja sortu je víceméně jednoduchý. Pokud má pole na vstupu nejvýše jeden prvek, pak takové pole rovnou vrať. Jinak pole rozděl na dvě části tak, že první část 
  obsahuje minimum a druhá část obsahuje všechny ostatní prvky. Existuje-li více minimálních prvků se stejnou hodnotou, tak do první části dej libovolný z nich 
  "pokud bychom chtěli, aby byl náš algoritmus stabilní, volili bychom místo libovolného prvku, prvek první".footnote. Na obě části postupně rekurzivně zavolej 
  algoritmus znovu, a poté pouze "přilep" obě části za sebe a výsledné pole vrať.


  Než se podívame na korektnost, tak nejprve ukážeme, že se algoritmus vůbec zastaví "jinak korektnost nemá moc smysl zkoumat. I kdyby algoritmus vracel seřazené pole, výsledku bychom
  se nikdy nedočkali".footnote. Má-li pole na vstupu nejvýše jeden prvek, pak se algoritmus jistě zastaví, neboť takové pole rovnou vrátíme a žádnou další operaci již nevykonáváme. Má-li pole
  prvků více, tak najdeme minimum, vložíme ho do první části, a do druhé části dáme zbytek prvků. Algoritmus zavolaný rekurzivně na první část jistě skončí, neboť první část obsahuje
  pouze jeden prvek. Dále algoritmus voláme na druhou část, která má velikost o jedna menší než původní pole. Rekurzivně tak budeme postupně zmenšovat a zmenšovat druhou část, dokud i v 
  ní nezbude pouze jeden prvek "předpokládáme, že vstupní pole je konečné délky".footnote. Jakmile se tak stane, dělení skončí, a my začneme postupně vylézat z rekurze ven. Pokaždé, co
  vystoupáme o volání rekurze výš, provedeme akorát operaci přilepení obou částí, což je jistě operace konečná. Algoritmus je tedy konečný.
  

  To, že pole je na konci celého algoritmu správně seřazené, a tedy že je algoritmus korektní, dokážeme indukcí podle hloubky stromu "mimochodem základní poučka
  platí, že tam, kde je rekurze, je i indukce, a naopak :)".footnote. Začneme od listů a induktivně budeme postupovat směrem ke kořeni. Protože pole o jendom prvku je
  triviálně správně seřazené, máme základní krok dokázaný. V indukčním kroku předpokládejme, že obě části pole (oba synové ve stromu dělení) jsou již správně
  seřazené. Náš algoritmus v tomto případě přilepí seřazenou první část za seřazenou druhou část a výsledné pole vrátí. To že je celé pole na konci skutečně seřazené plyne
  z toho, jak jsme pole dělili, a dokážeme to pro ukázku sporem. Předpokládejme, že pole na konci seřazené není. Existují tedy dva prvky $a$ a $b$ takové, že $a > b$, ale
  $a$ je v seřazeném poli dříve, než prvek $b$. Oba prvky se nemohly nacházet pouze v jedné části, protože jinak by nebyla správně seřazená ani ona samotná část, což je ve sporu
  s indukčním předpokladem. Nutně se tedy muselo $a$ nacházet v části první a $b$ v části druhé. To je ale spor s tím, jak jsme pole na části dělili, protože $b < a$ a $a$ se nachází
  v první části.


  Konečnost a korektnost máme hotovou, nyní by nás zajímalo, jak rychle samotný algoritmus vůbec poběží. Pozorný čtenář už si jistě všiml jisté podobnosti s jiným, již uvedeným,
  řadícím algoritmem. Skutečně, tento algoritmus je pouze rekurzivní verzí Select sortu, a běží v nejhorším případě v čase $\lanO(n^2)$.


  Proč je algoritmus tak pomalý? Nemohli bychom ho zrychlit? Problém je v tom, jak pole dělíme. Tím, že do jedné části vkládáme vždy pouze jeden prvek, je hloubka stromu
  dělení % TO CHCE ŘÍCT NĚJAK JINAK
  přímo úměrná počtu prvků v původním poli, tj. $\lanO(n)$.

  !tikz:
    filename: marakuja-sort-figure

      \node {n}
        child {node {1}}
        child {node {$n - 1$}
          child {node {1}}
          child {node {$n - 2$}
            child {node {1}}
            child {node {$n - 3$}
              child {node {1}}
              child {node {$\dots$}
                child {node {1}}
                child {node {$n - (n - 1)$}}
              }
            }
          }
        };

  
  Nezapomeňte, že v každém kroku rekurze hledáme minimum, kroků rekurze máme $\lanO(n)$ a součet aritmetické posloupnosti $n$ prvků je $\lanO(n^2)$. Kdyby se nám podařilo zmenšit hloubku
  stromu dělení, značně bychom si pomohli. Časová složitost našeho algoritmu v nejhorším případě je totiž rovna $\O(n \cdot h) "rozmyslete!".footnote, kde $h$ je hloubka stromu dělení
  (nezapomeňte, že hloubka je také závislá na počtu prvků $n$!). Předtím než si přečtete následující "a mnohem známější".footnote algoritmus, zkuste se zamyslet, jak bychom museli pole dělit,
  abychom snížili celkovou hloubku stromu dělení.


.Procedure:
  title: Quick sort
  index: sort!quick sort

  Quicksort je značně podobný Marakuja sortu. Vlastně jediným rozdílem mezi oběma algoritmy je způsob dělení. V quicksortu (nějak) vybereme jeden prvek $a_i$ a pole rozdělíme
  na dvě části tak, že v první části budou všechny prvky menší než $a_i$ a do druhé části dáme zbytek. Zbytek algoritmu je stejný. Základní motivací pro quicksort je co nejvíce
  zmenšit hloubku stromu. Ideálně bychom chtěli pole rozdělit vždy na cca stejně velké části. Tím hloubku (binárního) stromu snížíme na $\Theta(log n)$ a časová složitost bude pak
  \textbf{v průměrném případě} $\lanO(n \log n)$ (v nejhorším případě si bohužel nepomůžeme, jak uvidíte v následující analýze algoritmu).

  
  !tikz:
    filename: quicksort-figure

      \node {n}
        child {node {$\frac{n}{2}$}
          child {node {$\frac{n}{4}$}
            child {node {$\frac{n}{8}$}
            }
            child {node {$\frac{n}{8}$}
            }
          }
          child {node {$\frac{n}{4}$}
            child {node {$\frac{n}{8}$}
            }
            child {node {$\frac{n}{8}$}
            }
          }
        }
        child {node {$\frac{n}{2}$}
          child {node {$\frac{n}{4}$}
            child {node {$\frac{n}{8}$}
            }
            child {node {$\frac{n}{8}$}
            }
          }
          child {node {$\frac{n}{4}$}
            child {node {$\frac{n}{8}$}
            }
            child {node {$\frac{n}{8}$}
            }
          }
        };

.Solution:
  title: Časová složitost quicksortu

  Začneme časovou složitostí v nejhorším případě. Pokud bychom za $a_i$ volili vždy druhý nejmenší prvek, pak by časová složitost v nejhorším případě byla stejná jako
  u Marakuja sortu, tedy $\lanO(n^2)$.

  Pokud budeme pokaždé dělit pole na dvě stejně velké části, tak budeme dělit $\log_2(n)$ krát než dojdeme k polím o jednom prvku.

  Na $k$-té vrstvě tohoto stromu se nachází $2^{k-1}$ částí. A pro každou z nich provedeme práci $\lanO(\frac{n}{2^{k-1}})$. Takže
  na $k$-té vrstvě provedeme $2^{k-1} \cdot \lanO(\frac{n}{2^{k-1}})$ práce. To vychází na $\lanO(n)$ času ztráveného na jedné vrstvě.
  Vrstev je jak už jsme si řekli $\log_2(n)$, takže výsledná časová složitost je $\lanO(n*\log(n))$.


  Nyní si povíme o výběru prvku $a_i$. Tomuto prvku budeme říkat \textbf{pivot}. Byli bychom nejraději, kdyby pivot byl \textbf{medián}. 
  Dle definice je medián prvek na prostřední pozici \textbf{seřazeného pole}. Má-li pole sudý počet prvků, pak si vybereme buď ten
  trochu víc na pravo nebo trochu víc nalevo...\textbf{tohle se musí nutně přepsat, hnus}. Medián nám pole dle úvodu tohoto důkazu rozdělí pole přibližně na dvě stejně velké části.
  

  Stačí tedy najít medián v neseřazeném poli a máme vyhráno. Bohužel pro nás, najít medián není výpočetně snadná operace. (Ví se že medián nelze najít rychleji než v čase
  $\lanO(n\cdot\log(n))?). Místo toho můžeme využít takzvaných skoromediánů. "Skoromediány".notion.1 jsou prvky, které se nachází ve druhé a třetí čtvrtině seřazeného pole.
  Pokud vybereme za pivot skoromedián, tak se pole rozdělí na dvě části o $\frac{n}{4}$ a $\frac{3n}{4}$ prvcích. Se velikost pole co musíme seřadit bude na každé vrstvě zmenšovat
  alespoň na $\frac34n$. Než se dostaneme k poli o jednom prvku, tak to bude trvat $\log_\frac43 n$ což je $\lanO(\log n)$. Na jedné vrstvě provedeme
  $\lanO(n)$ operací, takže výsledek je stále $\lanO(n\log n)$. Jenže na výběr skoromediánu jako pivota máme jenom $50\%$ pravděpodobnost, takže v půlce případů,
  vybereme špatného pivota. To ale neznamená nic jiného, než že bude strom rozdělování pole dvakrát tak velký, což pořád vede na $\lanO(n\log n)$ časovou složitost.
  To nám bude stačit, pokud se chcete dovědět více, podívejte se na kapitolu 11 "Průvodce"@2
    1:
      index: skoromedián
    2:
      link: http://pruvodce.ucw.cz/

.Remark
  title: Další popisy quicksortu

  V jiné literatuře můžete narazit i na jiné popisy quicksortu. Jedním z nich je např. ten, kdy pole místo dvou částí, rozdělíme na tři. (Nějak) zvolíme jeden prvek $a_i$
  a pole rozdělíme na část s prvky menší než $a_i$, část s prvky rovnými $a_i$ a na část s prvky většími než $a_i$. Rekurzivně řadíme pouze první a poslední část, neboť prostřední
  část je již triviálně seřazená. V operaci slepování pak pouze slepíme první, prostřední a poslední část za sebe. Prvek $a_i$ nazýváme \textbf{pivot}.

